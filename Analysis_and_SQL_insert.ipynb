{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are many missing values and they can be taken from the internet. I can proceed in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_netflix_movie_data = pd.read_csv('./Datasets/netflix_titles.csv', lineterminator = '\\n')\n",
    "df_prime_movie_data = pd.read_csv('./Datasets/amazon_prime_titles.csv', lineterminator = '\\n')\n",
    "df_disney_plus_movie_data = pd.read_csv('./Datasets/disney_plus_titles.csv', lineterminator = '\\n')\n",
    "df_hulu_movie_data = pd.read_csv('./Datasets/hulu_titles.csv', lineterminator = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_netflix = df_netflix_movie_data.isna().sum()\n",
    "print(missing_value_netflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Netflix DataFrame:\", df_netflix_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_prime = df_prime_movie_data.isna().sum()\n",
    "print(missing_value_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Prime Video DataFrame:\", df_prime_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_disney_plus = df_disney_plus_movie_data.isna().sum()\n",
    "print(missing_value_disney_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Disney Plus DataFrame:\", df_disney_plus_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_hulu = df_hulu_movie_data.isna().sum()\n",
    "print(missing_value_hulu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Hulu DataFrame:\", df_hulu_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see clearly, above only columns director, cast, country, date_added, rating, duration. Rather than deleting them. If we can get those values back in the dataset it would be best.\n",
    "I can use information present on the website imdb to get the data and fill it. I can use web scrapping to get the data and fill it in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before that I can check data. One dataset row may have data of the for the other dataset missing row values. Let's first check for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Netflix DataFrame Column Data Types:\")\n",
    "print(df_netflix_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrime Video DataFrame Column Data Types:\")\n",
    "print(df_prime_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nDisney Plus DataFrame Column Data Types:\")\n",
    "print(df_disney_plus_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nHulu DataFrame Column Data Types:\")\n",
    "print(df_hulu_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I will be first converting the datatypes of the columns of the dataframes according to the requirement and then after that I will be tranferring data to the SQL. Through this I can ensure that there is no error while tranferring the data.\n",
    "\n",
    "Why I want to transfer the data to SQL?\n",
    "I currently want to fill the missing values of the columns director, cast, country, date_added, rating and duration in all datasets. But there are chances that missing value of 1 dataset might be present in another dataset. So for doing that in optimized manner I am using the SQL. \n",
    "Other approaches:\n",
    "\n",
    "- I can loop through datasets to find values. But this approach will take most time as there are 4 datasets and time taken will get to O($n^4$). This approach is not great.\n",
    "- I can use Hash tables for this but it will be take most of the memory for it. It would be better to go for Database as I would be taking it's help for data wrangling also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_netflix_movie_data, df_prime_movie_data, df_disney_plus_movie_data, df_hulu_movie_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataframes:\n",
    "    df[\"show_id\"] = df[\"show_id\"].astype(\"string\")  # Explicit string type\n",
    "    df[\"type\"] = df[\"type\"].astype(\"string\")\n",
    "    df[\"title\"] = df[\"title\"].astype(\"string\")\n",
    "    df[\"director\"] = df[\"director\"].astype(\"string\")\n",
    "    df[\"cast\"] = df[\"cast\"].astype(\"string\")  # Convert float64 to string\n",
    "    df[\"country\"] = df[\"country\"].astype(\"string\")\n",
    "    df[\"date_added\"] = pd.to_datetime(df[\"date_added\"], errors=\"coerce\")  # Convert to Date\n",
    "    df[\"release_year\"] = df[\"release_year\"].astype(\"Int64\")  # Explicitly use Int64\n",
    "    df[\"rating\"] = df[\"rating\"].astype(\"string\")\n",
    "    df[\"duration\"] = df[\"duration\"].astype(\"string\")\n",
    "    df[\"listed_in\"] = df[\"listed_in\"].astype(\"string\")\n",
    "    df[\"description\"] = df[\"description\"].astype(\"string\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types for DataFrame 1:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n",
      "\n",
      "Data Types for DataFrame 2:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n",
      "\n",
      "Data Types for DataFrame 3:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n",
      "\n",
      "Data Types for DataFrame 4:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verify Data Types\n",
    "for i, df in enumerate(dataframes, start=1):\n",
    "    print(f\"\\nData Types for DataFrame {i}:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"Himanshu\",\n",
    "    password=\"Hustling@2000\",\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP DATABASE IF EXISTS movies_db\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS movies_db;\")\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"Himanshu\",\n",
    "    password=\"Hustling@2000\",\n",
    "    database=\"movies_db\"\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS movies_data (\n",
    "        show_id VARCHAR(255),\n",
    "        type VARCHAR(50),\n",
    "        title VARCHAR(500),\n",
    "        director VARCHAR(300),\n",
    "        cast VARCHAR(2000),\n",
    "        country VARCHAR(100),\n",
    "        date_added DATE,\n",
    "        release_year YEAR,\n",
    "        rating VARCHAR(50),\n",
    "        duration VARCHAR(100),\n",
    "        listed_in VARCHAR(300),\n",
    "        description TEXT,\n",
    "        PRIMARY KEY (title, release_year)\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so the database has been created. But I have to make sure that all the data goes properly inside the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize failed_inserts as an empty DataFrame with correct columns\n",
    "df_failed_inserts = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Assuming you have already established a connection and cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert each row, perform verification, and handle errors\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        # Convert NaN values to None, which can be handled by MySQL\n",
    "        row = row.where(pd.notna(row), None)\n",
    "\n",
    "        # Log the row that is about to be inserted\n",
    "        print(f\"\\nInserting row with title: {row['title']} and release_year: {row['release_year']}\")\n",
    "\n",
    "        # Insert data into the database\n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO movies_data \n",
    "            (show_id, type, title, director, cast, country, date_added, release_year, \n",
    "             rating, duration, listed_in, description)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(insert_query, (\n",
    "            row[\"show_id\"], row[\"type\"], row[\"title\"], row[\"director\"],\n",
    "            row[\"cast\"], row[\"country\"], row[\"date_added\"], row[\"release_year\"],\n",
    "            row[\"rating\"], row[\"duration\"], row[\"listed_in\"], row[\"description\"]\n",
    "        ))\n",
    "        conn.commit()  # Commit the transaction\n",
    "\n",
    "        # Log success\n",
    "        print(f\"Data for '{row['title']}' inserted successfully.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        # Log the error\n",
    "        print(f\"\\nError occurred while inserting data: {err}\")\n",
    "        print(f\"Row causing the error: \\n{row.to_frame().T}\")  # Print row in tabular format\n",
    "\n",
    "        # Append failed row properly to df_failed_inserts\n",
    "        df_failed_inserts = pd.concat([df_failed_inserts, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Print failed insertions, if any\n",
    "if not df_failed_inserts.empty:\n",
    "    print(\"\\nFailed Insertions due to duplicates or errors: \")\n",
    "    print(df_failed_inserts)\n",
    "else:\n",
    "    print(\"\\nAll rows inserted successfully without errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_failed_inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated missing values for: Trolls: TrollsTopia (2020)\n",
      "No matching record found in database for: Monos (2019)\n",
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "# Function to update the database with missing values filled\n",
    "def update_missing_values(row):\n",
    "    try:\n",
    "        # Extract the row data from the database using Title and Release Year\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT * FROM movies_data WHERE title = %s AND release_year = %s\n",
    "        \"\"\", (row[\"title\"], row[\"release_year\"]))\n",
    "        \n",
    "        db_row = cursor.fetchone()\n",
    "\n",
    "        if db_row:\n",
    "            # Convert fetched row into a dictionary\n",
    "            columns = [\"show_id\", \"type\", \"title\", \"director\", \"cast\", \"country\", \n",
    "                       \"date_added\", \"release_year\", \"rating\", \"duration\", \n",
    "                       \"listed_in\", \"description\"]\n",
    "            db_data = dict(zip(columns, db_row))\n",
    "\n",
    "            # Compare each element and fill missing values\n",
    "            updated_data = {}\n",
    "            for col in columns:\n",
    "                if pd.isna(row[col]) or row[col] is None:\n",
    "                    updated_data[col] = db_data[col]  # Fill missing value from DB\n",
    "                else:\n",
    "                    updated_data[col] = row[col]  # Keep the original value if not missing\n",
    "            \n",
    "            # If any values were updated, modify the database\n",
    "            update_query = \"\"\"\n",
    "                UPDATE movies_data\n",
    "                SET show_id = %s, type = %s, director = %s, cast = %s, country = %s, \n",
    "                    date_added = %s, rating = %s, duration = %s, listed_in = %s, \n",
    "                    description = %s\n",
    "                WHERE title = %s AND release_year = %s\n",
    "            \"\"\"\n",
    "            cursor.execute(update_query, (\n",
    "                updated_data[\"show_id\"], updated_data[\"type\"], updated_data[\"director\"], \n",
    "                updated_data[\"cast\"], updated_data[\"country\"], updated_data[\"date_added\"], \n",
    "                updated_data[\"rating\"], updated_data[\"duration\"], updated_data[\"listed_in\"], \n",
    "                updated_data[\"description\"], updated_data[\"title\"], updated_data[\"release_year\"]\n",
    "            ))\n",
    "            conn.commit()\n",
    "            print(f\"Updated missing values for: {row['title']} ({row['release_year']})\")\n",
    "        else:\n",
    "            print(f\"No matching record found in database for: {row['title']} ({row['release_year']})\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error updating {row['title']} ({row['release_year']}): {err}\")\n",
    "\n",
    "# Loop through each row in the dataframe and process\n",
    "for _, row in df_failed_inserts.iterrows():\n",
    "    update_missing_values(row)\n",
    "\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
