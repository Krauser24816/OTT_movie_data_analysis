{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are many missing values and they can be taken from the internet. I can proceed in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_netflix_movie_data = pd.read_csv('./Datasets/netflix_titles.csv', lineterminator = '\\n')\n",
    "df_prime_movie_data = pd.read_csv('./Datasets/amazon_prime_titles.csv', lineterminator = '\\n')\n",
    "df_disney_plus_movie_data = pd.read_csv('./Datasets/disney_plus_titles.csv', lineterminator = '\\n')\n",
    "df_hulu_movie_data = pd.read_csv('./Datasets/hulu_titles.csv', lineterminator = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_netflix_movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_netflix = df_netflix_movie_data.isna().sum()\n",
    "print(missing_value_netflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Netflix DataFrame:\", df_netflix_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_prime = df_prime_movie_data.isna().sum()\n",
    "print(missing_value_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Prime Video DataFrame:\", df_prime_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_disney_plus = df_disney_plus_movie_data.isna().sum()\n",
    "print(missing_value_disney_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Disney Plus DataFrame:\", df_disney_plus_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_hulu = df_hulu_movie_data.isna().sum()\n",
    "print(missing_value_hulu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records in Hulu DataFrame:\", df_hulu_movie_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see clearly, above only columns director, cast, country, date_added, rating, duration. Rather than deleting them. If we can get those values back in the dataset it would be best.\n",
    "I can use information present on the website imdb to get the data and fill it. I can use web scrapping to get the data and fill it in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before that I can check data. One dataset row may have data of the for the other dataset missing row values. Let's first check for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Netflix DataFrame Column Data Types:\")\n",
    "print(df_netflix_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrime Video DataFrame Column Data Types:\")\n",
    "print(df_prime_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nDisney Plus DataFrame Column Data Types:\")\n",
    "print(df_disney_plus_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nHulu DataFrame Column Data Types:\")\n",
    "print(df_hulu_movie_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I will be first converting the datatypes of the columns of the dataframes according to the requirement and then after that I will be tranferring data to the SQL. Through this I can ensure that there is no error while tranferring the data.\n",
    "\n",
    "Why I want to transfer the data to SQL?\n",
    "I currently want to fill the missing values of the columns director, cast, country, date_added, rating and duration in all datasets. But there are chances that missing value of 1 dataset might be present in another dataset. So for doing that in optimized manner I am using the SQL. \n",
    "Other approaches:\n",
    "\n",
    "- I can loop through datasets to find values. But this approach will take most time as there are 4 datasets and time taken will get to O($n^4$). This approach is not great.\n",
    "- I can use Hash tables for this but it will be take most of the memory for it. It would be better to go for Database as I would be taking it's help for data wrangling also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_netflix_movie_data, df_prime_movie_data, df_disney_plus_movie_data, df_hulu_movie_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets\n",
    "df_combined = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select one row where any value is null\n",
    "row_with_any_null = df_combined[df_combined.isnull().any(axis=1)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(row_with_any_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select one row where specific columns are null\n",
    "row_with_all_specific_null = df_combined[\n",
    "    df_combined[['director', 'cast', 'country', 'date_added', 'rating', 'duration', 'description']].isnull().all(axis=1)\n",
    "].head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [show_id, type, title, director, cast, country, date_added, release_year, rating, duration, listed_in, description]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(row_with_all_specific_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select rows where only one column at a time is null\n",
    "rows_with_one_null = []\n",
    "columns_to_check = ['director', 'cast', 'country', 'date_added', 'rating', 'duration', 'description']\n",
    "\n",
    "for col in columns_to_check:\n",
    "    row = df_combined[df_combined[col].isnull() & df_combined.drop(columns=[col]).notnull().all(axis=1)].head(1)\n",
    "    rows_with_one_null.append(row)\n",
    "\n",
    "# Combine all selected rows into a new DataFrame\n",
    "df_missing_data_check = pd.concat([row_with_any_null, row_with_all_specific_null] + rows_with_one_null, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resulting DataFrame\n",
    "display(df_missing_data_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data_check.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Updated country column to keep only the first country for each movie.\n"
     ]
    }
   ],
   "source": [
    "# Ensure country column contains only one country per movie\n",
    "for df in dataframes:\n",
    "    if 'country' in df.columns:\n",
    "        df['country'] = df['country'].astype(str).apply(lambda x: x.split(',')[0].strip() if pd.notnull(x) else x)\n",
    "\n",
    "print(\"[INFO] Updated country column to keep only the first country for each movie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataframes:\n",
    "    df[\"show_id\"] = df[\"show_id\"].astype(\"string\")  # Explicit string type\n",
    "    df[\"type\"] = df[\"type\"].astype(\"string\")\n",
    "    df[\"title\"] = df[\"title\"].astype(\"string\")\n",
    "    df[\"director\"] = df[\"director\"].astype(\"string\")\n",
    "    df[\"cast\"] = df[\"cast\"].astype(\"string\")  # Convert float64 to string\n",
    "    df[\"country\"] = df[\"country\"].astype(\"string\")\n",
    "    df[\"date_added\"] = pd.to_datetime(df[\"date_added\"], errors=\"coerce\")  # Convert to Date\n",
    "    df[\"release_year\"] = df[\"release_year\"].astype(\"Int64\")  # Explicitly use Int64\n",
    "    df[\"rating\"] = df[\"rating\"].astype(\"string\")\n",
    "    df[\"duration\"] = df[\"duration\"].astype(\"string\")\n",
    "    df[\"listed_in\"] = df[\"listed_in\"].astype(\"string\")\n",
    "    df[\"description\"] = df[\"description\"].astype(\"string\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types for DataFrame 1:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n",
      "\n",
      "Data Types for DataFrame 2:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n",
      "\n",
      "Data Types for DataFrame 3:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n",
      "\n",
      "Data Types for DataFrame 4:\n",
      " show_id         string[python]\n",
      "type            string[python]\n",
      "title           string[python]\n",
      "director        string[python]\n",
      "cast            string[python]\n",
      "country         string[python]\n",
      "date_added      datetime64[ns]\n",
      "release_year             Int64\n",
      "rating          string[python]\n",
      "duration        string[python]\n",
      "listed_in       string[python]\n",
      "description     string[python]\n",
      "dtype: object\n",
      "\n",
      " Total number of rows:22998\n"
     ]
    }
   ],
   "source": [
    "# Verify Data Types\n",
    "total_num_of_rows = 0\n",
    "for i, df in enumerate(dataframes, start=1):\n",
    "    total_num_of_rows += len(df)\n",
    "    print(f\"\\nData Types for DataFrame {i}:\\n\", df.dtypes)\n",
    "\n",
    "print(f\"\\n Total number of rows:{total_num_of_rows}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataframes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"Himanshu\",\n",
    "    password=\"12345\",\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"USE movies_db\")  # Switch to a different DB\n",
    "cursor.execute(\"DROP DATABASE IF EXISTS movies_db\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS movies_db;\")\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"Himanshu\",\n",
    "    password=\"12345\",\n",
    "    database=\"movies_db\"\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS movies_data (\n",
    "        show_id VARCHAR(255),\n",
    "        type VARCHAR(50),\n",
    "        title VARCHAR(500),\n",
    "        director VARCHAR(300),\n",
    "        cast VARCHAR(2000),\n",
    "        country VARCHAR(100),\n",
    "        date_added DATE,\n",
    "        release_year YEAR,\n",
    "        rating VARCHAR(50),\n",
    "        duration VARCHAR(100),\n",
    "        listed_in VARCHAR(300),\n",
    "        description TEXT,\n",
    "        PRIMARY KEY (title, release_year)\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so the database has been created. But I have to make sure that all the data goes properly inside the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store failed insertions\n",
    "failed_insertions = []\n",
    "\n",
    "# Define table name\n",
    "table_name = \"movies_data\"\n",
    "\n",
    "# Iterate through the DataFrame for insertion\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # Convert NaN to None for SQL compatibility\n",
    "        values = [None if pd.isna(row[col]) else row[col] for col in df.columns]\n",
    "        \n",
    "        # Construct INSERT query dynamically\n",
    "        columns_str = \", \".join(df.columns)\n",
    "        placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "        insert_query = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})\"\n",
    "\n",
    "        # Log data before insertion\n",
    "        print(f\"[INFO] Inserting data: {values}\")\n",
    "\n",
    "        # Execute insertion\n",
    "        cursor.execute(insert_query, tuple(values))\n",
    "        conn.commit()\n",
    "        print(\"[SUCCESS] Data inserted successfully!\")\n",
    "\n",
    "        # Fetch inserted data to validate\n",
    "        primary_keys = [\"title\", \"release_year\"]\n",
    "        where_clause = \" AND \".join([f\"{pk} = %s\" for pk in primary_keys])\n",
    "        select_query = f\"SELECT * FROM {table_name} WHERE {where_clause}\"\n",
    "\n",
    "        cursor.execute(select_query, tuple(row[pk] for pk in primary_keys))\n",
    "        db_record = cursor.fetchone()\n",
    "\n",
    "        # If no matching record found, log missing data\n",
    "        if not db_record:\n",
    "            print(f\"[ERROR] Data missing from DB after insertion: {row[primary_keys].to_dict()}\")\n",
    "            failed_insertions.append({**row.to_dict(), \"error\": \"Data missing after insertion\"})\n",
    "            continue\n",
    "\n",
    "        # Compare column values\n",
    "        db_dict = dict(zip(df.columns, db_record))\n",
    "        for col in df.columns:\n",
    "            df_value = str(row[col]).strip() if pd.notna(row[col]) else None\n",
    "            db_value = str(db_dict[col]).strip() if db_dict[col] is not None else None\n",
    "\n",
    "            if df_value != db_value:\n",
    "                print(f\"[WARNING] Mismatch found in column '{col}': DF='{df_value}', DB='{db_value}'\")\n",
    "                failed_insertions.append({**row.to_dict(), \"error\": f\"Mismatch in column '{col}'\"})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to insert data: {row.to_dict()}\\nReason: {str(e)}\")\n",
    "        failed_insertions.append({**row.to_dict(), \"error\": str(e)})\n",
    "        conn.rollback()  # Rollback on failure\n",
    "\n",
    "# Convert failed insertions into DataFrame\n",
    "df_failed_insertions = pd.DataFrame(failed_insertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize failed_inserts as an empty DataFrame with correct columns\n",
    "df_failed_inserts = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Assuming you have already established a connection and cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert each row, perform verification, and handle errors\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        # Convert NaN values to None, which can be handled by MySQL\n",
    "        row = row.where(pd.notna(row), None)\n",
    "\n",
    "        # Log the row that is about to be inserted\n",
    "        print(f\"\\nInserting row with title: {row['title']} and release_year: {row['release_year']}\")\n",
    "\n",
    "        # Insert data into the database\n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO movies_data \n",
    "            (show_id, type, title, director, cast, country, date_added, release_year, \n",
    "             rating, duration, listed_in, description)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(insert_query, (\n",
    "            row[\"show_id\"], row[\"type\"], row[\"title\"], row[\"director\"],\n",
    "            row[\"cast\"], row[\"country\"], row[\"date_added\"], row[\"release_year\"],\n",
    "            row[\"rating\"], row[\"duration\"], row[\"listed_in\"], row[\"description\"]\n",
    "        ))\n",
    "        conn.commit()  # Commit the transaction\n",
    "\n",
    "        # Log success\n",
    "        print(f\"Data for '{row['title']}' inserted successfully.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        # Log the error\n",
    "        print(f\"\\nError occurred while inserting data: {err}\")\n",
    "        print(f\"Row causing the error: \\n{row.to_frame().T}\")  # Print row in tabular format\n",
    "\n",
    "        # Append failed row properly to df_failed_inserts\n",
    "        df_failed_inserts = pd.concat([df_failed_inserts, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Print failed insertions, if any\n",
    "if not df_failed_inserts.empty:\n",
    "    print(\"\\nFailed Insertions due to duplicates or errors: \")\n",
    "    print(df_failed_inserts)\n",
    "else:\n",
    "    print(\"\\nAll rows inserted successfully without errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_failed_inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Data loss detected!\n",
      "Expected in DB: 22996, Found in DB: 3071\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute Total Rows Before Insertion\n",
    "total_num_of_rows_in_df = (\n",
    "    len(df_netflix_movie_data) + \n",
    "    len(df_prime_movie_data) + \n",
    "    len(df_disney_plus_movie_data) + \n",
    "    len(df_hulu_movie_data)\n",
    ")\n",
    "\n",
    "# 2. Remove Duplicate Rows That Failed to Insert\n",
    "total_num_of_rows_expected_in_db = total_num_of_rows_in_df - len(df_failed_inserts)\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM movies_data;\")\n",
    "total_num_of_rows_in_db = cursor.fetchone()[0]\n",
    "\n",
    "# 4. Validate Data Transfer\n",
    "if total_num_of_rows_expected_in_db == total_num_of_rows_in_db:\n",
    "    print(\"[SUCCESS] No data loss!\")\n",
    "else:\n",
    "    print(\"[WARNING] Data loss detected!\")\n",
    "    print(f\"Expected in DB: {total_num_of_rows_expected_in_db}, Found in DB: {total_num_of_rows_in_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated missing values for: Trolls: TrollsTopia (2020)\n",
      "No matching record found in database for: Monos (2019)\n",
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "# Function to update the database with missing values filled\n",
    "def update_missing_values(row):\n",
    "    try:\n",
    "        # Extract the row data from the database using Title and Release Year\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT * FROM movies_data WHERE title = %s AND release_year = %s\n",
    "        \"\"\", (row[\"title\"], row[\"release_year\"]))\n",
    "        \n",
    "        db_row = cursor.fetchone()\n",
    "\n",
    "        if db_row:\n",
    "            # Convert fetched row into a dictionary\n",
    "            columns = [\"show_id\", \"type\", \"title\", \"director\", \"cast\", \"country\", \n",
    "                       \"date_added\", \"release_year\", \"rating\", \"duration\", \n",
    "                       \"listed_in\", \"description\"]\n",
    "            db_data = dict(zip(columns, db_row))\n",
    "\n",
    "            # Compare each element and fill missing values\n",
    "            updated_data = {}\n",
    "            for col in columns:\n",
    "                if pd.isna(row[col]) or row[col] is None:\n",
    "                    updated_data[col] = db_data[col]  # Fill missing value from DB\n",
    "                else:\n",
    "                    updated_data[col] = row[col]  # Keep the original value if not missing\n",
    "            \n",
    "            # If any values were updated, modify the database\n",
    "            update_query = \"\"\"\n",
    "                UPDATE movies_data\n",
    "                SET show_id = %s, type = %s, director = %s, cast = %s, country = %s, \n",
    "                    date_added = %s, rating = %s, duration = %s, listed_in = %s, \n",
    "                    description = %s\n",
    "                WHERE title = %s AND release_year = %s\n",
    "            \"\"\"\n",
    "            cursor.execute(update_query, (\n",
    "                updated_data[\"show_id\"], updated_data[\"type\"], updated_data[\"director\"], \n",
    "                updated_data[\"cast\"], updated_data[\"country\"], updated_data[\"date_added\"], \n",
    "                updated_data[\"rating\"], updated_data[\"duration\"], updated_data[\"listed_in\"], \n",
    "                updated_data[\"description\"], updated_data[\"title\"], updated_data[\"release_year\"]\n",
    "            ))\n",
    "            conn.commit()\n",
    "            print(f\"Updated missing values for: {row['title']} ({row['release_year']})\")\n",
    "        else:\n",
    "            print(f\"No matching record found in database for: {row['title']} ({row['release_year']})\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error updating {row['title']} ({row['release_year']}): {err}\")\n",
    "\n",
    "# Loop through each row in the dataframe and process\n",
    "for _, row in df_failed_inserts.iterrows():\n",
    "    update_missing_values(row)\n",
    "\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store records with mismatched data\n",
    "mismatch_data = []\n",
    "\n",
    "# Get the list of columns in df_missing_data_check (excluding primary keys)\n",
    "columns_to_check = list(df_missing_data_check.columns)\n",
    "columns_to_check.remove(\"title\")\n",
    "columns_to_check.remove(\"release_year\")\n",
    "\n",
    "# Iterate through df_missing_data_check to verify each record\n",
    "for index, row in df_missing_data_check.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    release_year = row[\"release_year\"]\n",
    "\n",
    "    # Query to fetch the corresponding row from the database\n",
    "    query = f\"\"\"\n",
    "    SELECT {', '.join(columns_to_check)} FROM your_table_name \n",
    "    WHERE title = %s AND release_year = %s;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(query, (title, release_year))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    # If no matching record found, log as missing\n",
    "    if not result:\n",
    "        mismatch_data.append({\n",
    "            \"title\": title,\n",
    "            \"release_year\": release_year,\n",
    "            \"column\": \"ALL\",\n",
    "            \"df_value\": \"Exists in DF\",\n",
    "            \"db_value\": \"Missing in DB\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Compare each column value\n",
    "    db_row = dict(zip(columns_to_check, result))\n",
    "    for col in columns_to_check:\n",
    "        df_value = str(row[col]).strip() if pd.notna(row[col]) else None\n",
    "        db_value = str(db_row[col]).strip() if db_row[col] is not None else None\n",
    "\n",
    "        if df_value != db_value:\n",
    "            mismatch_data.append({\n",
    "                \"title\": title,\n",
    "                \"release_year\": release_year,\n",
    "                \"column\": col,\n",
    "                \"df_value\": row[col],\n",
    "                \"db_value\": db_row[col]\n",
    "            })\n",
    "\n",
    "# Convert mismatched records to a DataFrame\n",
    "df_mismatched_data = pd.DataFrame(mismatch_data, columns=[\"title\", \"release_year\", \"column\", \"df_value\", \"db_value\"])\n",
    "\n",
    "# Display results\n",
    "if df_mismatched_data.empty:\n",
    "    print(\"[SUCCESS] All records in df_missing_data_check match exactly in the database. 🎉\")\n",
    "else:\n",
    "    print(\"[WARNING] Some records in df_missing_data_check do not match the database!\")\n",
    "    print(df_mismatched_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
